# %%
print("STEP 1 ì™„ë£Œ: corpus ìƒì„±")
print("STEP 1 ì™„ë£Œ: corpus ìƒì„±")

# %%
import pandas as pd
import string
import numpy as np

from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from nltk.tokenize import TreebankWordTokenizer

from gensim import corpora, models
from gensim.models import CoherenceModel

import matplotlib.pyplot as plt


# %%
import pyLDAvis
import pyLDAvis.gensim_models as gensimvis

# %%
from gensim.models import LdaModel, CoherenceModel

def compute_coherence_only(dictionary, corpus, texts, start, limit, step):
    topic_nums = []
    coherence_scores = []

    for num_topics in range(start, limit, step):
        lda_model = LdaModel(
            corpus=corpus,
            id2word=dictionary,
            num_topics=num_topics,
            random_state=42,
            passes=5,
            iterations=100
        )

        coherence_model = CoherenceModel(
            model=lda_model,
            texts=texts,
            dictionary=dictionary,
            coherence='c_v'
        )

        topic_nums.append(num_topics)
        coherence_scores.append(coherence_model.get_coherence())

    return topic_nums, coherence_scores


# %%
df = pd.read_excel(
    "/Users/pinetree/tactile-design-LDA/20251230_scopus_733.xlsx"
)


# %%
df["text"] = (
    df["Title"].fillna("") + " " +
    df["Author Keywords"].fillna("") + " " +
    df["Abstract"].fillna("")
)


# %%
# ê¸°ë³¸ ì„¤ì •
stop_words = set(stopwords.words("english"))
stemmer = PorterStemmer()
tokenizer = TreebankWordTokenizer()

# ì‚¬ìš©ì ì •ì˜ stopwords (ì›í˜•)
custom_stop = {
    "study","paper","result","results","method","methods","analysis","approach",
    "conclusion","implication","implications","introduction","discussion",
    "based","using","use","used","new","one","two","three","within","across",
    "haptic","touch","tactile","design","process","explore","common","user",
    "work","practice","model","product","system","object","support","effect",
    "research","article","publisher","scopus","elsevier","make","tool","group","show","differ",
    "also","may","might","many","much","however","therefore","thus",
    "present","reflect","task","people","de","et",
    "develop","provide","improve","perform","date","data"
}

# âœ… í•µì‹¬: stem ê¸°ì¤€ stopwordsê¹Œì§€ ë¯¸ë¦¬ ìƒì„±
custom_stop_stem = {stemmer.stem(w) for w in custom_stop}
stop_words_stem = {stemmer.stem(w) for w in stop_words}

# ì›í˜• stopwords
all_stop = stop_words | custom_stop

def preprocess_text(text):
    text = str(text).lower()
    text = text.translate(str.maketrans("", "", string.punctuation))
    
    tokens = tokenizer.tokenize(text)
    tokens = [t for t in tokens if t.isalpha()]

    cleaned = []
    for t in tokens:
        t_stem = stemmer.stem(t)

        # âœ… ì›í˜• ê¸°ì¤€ ì œê±°
        if t in all_stop:
            continue

        # âœ… stem ê¸°ì¤€ ì œê±° (ë©”íƒ€ ë‹¨ì–´ ì”ì¡´ ë¬¸ì œ í•´ê²° í•µì‹¬)
        if t_stem in custom_stop_stem:
            continue
        if t_stem in stop_words_stem:
            continue

        cleaned.append(t_stem)

    return cleaned


# %%
df["tokens"] = df["text"].apply(preprocess_text)

print("ë¬¸ì„œ ìˆ˜:", len(df))
print("í‰ê·  í† í° ìˆ˜:", df["tokens"].apply(len).mean())


# %%
dictionary = corpora.Dictionary(df["tokens"])
dictionary.filter_extremes(no_below=5, no_above=0.5)

corpus = [dictionary.doc2bow(text) for text in df["tokens"]]

print(f"Dictionary size: {len(dictionary)}")


# %%
lda = LdaMulticore(
    corpus=corpus,
    id2word=dictionary,
    num_topics=10,
    passes=5,
    iterations=100,
    chunksize=2000,
    eval_every=None,
    workers=4,
    random_state=42
)

# %%
# ì‚¬ì „ì— ì–´ë–¤ ë‹¨ì–´ê°€ ë‚¨ì•˜ëŠ”ì§€ í™•ì¸ (ë¹ˆë„ìˆ˜ ìƒìœ„ 50ê°œ)
token_counts = {}
for doc in corpus:
    for word_id, count in doc:
        token_counts[word_id] = token_counts.get(word_id, 0) + count

# ë‹¨ì–´ì™€ ë¹ˆë„ìˆ˜ ë§¤ì¹­í•´ì„œ ë³´ê¸°
sorted_tokens = sorted(token_counts.items(), key=lambda x: x[1], reverse=True)
for word_id, count in sorted_tokens[:30]:
    print(f"{dictionary[word_id]}: {count}")

# %%
import matplotlib.pyplot as plt

best_k = None
best_coherence = -1

ks = list(range(2, 10))      # 2~9
coherence_scores = []        # âœ… ë°˜ë“œì‹œ forë¬¸ ë°–ì—ì„œ 1ë²ˆë§Œ ì´ˆê¸°í™”

print("=== í† í”½ ìˆ˜ë³„ Coherence (c_v) ===")

for k in ks:
    lda_tmp = LdaModel(
        corpus=corpus,
        id2word=dictionary,
        num_topics=k,
        random_state=42,
        passes=5
    )

    coherence_model = CoherenceModel(
        model=lda_tmp,
        texts=df["tokens"],
        dictionary=dictionary,
        coherence="c_v"
    )

    coherence = coherence_model.get_coherence()
    print(f"num_topics = {k} | coherence = {coherence:.4f}")

    coherence_scores.append(coherence)

    if coherence > best_coherence:
        best_coherence = coherence
        best_k = k

print(f"\nìµœëŒ€ Coherence: {best_coherence:.4f} (í† í”½ ìˆ˜ = {best_k})")

# -----------------------------
# âœ… ê·¸ë˜í”„ (ì—¬ê¸°ë¶€í„° ë¶™ì´ë©´ ë¨)
# -----------------------------
plt.figure(figsize=(8, 5))
plt.plot(ks, coherence_scores, marker="o")
plt.scatter(best_k, best_coherence, s=80)   # ìµœëŒ€ê°’ ê°•ì¡°
plt.xlabel("Number of Topics")
plt.ylabel("Coherence Score (c_v)")
plt.title("Coherence Score by Number of Topics")
plt.xticks(ks)
plt.grid(True)
plt.show()


# %%
num_topics = 7 # â† coherence ê²°ê³¼ë¡œ ê²°ì •

lda_model = models.LdaModel(
    corpus=corpus,
    id2word=dictionary,
    num_topics=num_topics,
    random_state=42,
    passes=20,
    alpha="auto"
)


# %%
# í† í”½ë³„ ìƒìœ„ ë‹¨ì–´ ì¶œë ¥
for topic_id, topic_words in lda_model.print_topics(num_topics=num_topics, num_words=10):
    print(f"Topic {topic_id}: {topic_words}")


# %%
rows = []
topics = lda_model.show_topics(
    num_topics=num_topics,
    num_words=10,
    formatted=False
)

for topic_id, words in topics:
    for word, weight in words:
        rows.append({
            "topic": topic_id,
            "word": word,
            "weight": weight
        })

topic_df = pd.DataFrame(rows)
topic_df


# %% [markdown]
# ì¤‘ë³µí¬í•¨ë‹¨ì–´ìˆ˜

# %%
all_tokens = [token for doc in df["tokens"] for token in doc]
total_token_count = len(all_tokens)

print(f"ì¤‘ë³µ í¬í•¨ ë‹¨ì–´ ìˆ˜ (Total tokens): {total_token_count}")


# %% [markdown]
# ê³ ìœ ë‹¨ì–´ìˆ˜

# %%
unique_token_count = len(set(all_tokens))

print(f"ê³ ìœ  ë‹¨ì–´ ìˆ˜ (Unique tokens): {unique_token_count}")


# %% [markdown]
# ë¬¸ì„œ í‰ê·  ë‹¨ì–´ ìˆ˜ (ë…¼ë¬¸ì— ìì£¼ ì“°ì„)

# %%
avg_tokens_per_doc = df["tokens"].apply(len).mean()

print(f"ë¬¸ì„œë‹¹ í‰ê·  ë‹¨ì–´ ìˆ˜: {avg_tokens_per_doc:.2f}")


# %% [markdown]
# (ì„ íƒ) Dictionary ê¸°ì¤€ ì–´íœ˜ í¬ê¸°

# %%
print(f"LDA Dictionary ë‹¨ì–´ ìˆ˜: {len(dictionary)}")


# %% [markdown]
# perplexity ìµœì ê°’

# %%


def compute_perplexity_only(dictionary, corpus, start=2, limit=15, step=1):
    topic_nums = []
    perplexity_scores = []

    for k in range(start, limit + 1, step):
        lda = models.LdaModel(
            corpus=corpus,
            id2word=dictionary,
            num_topics=k,
            random_state=42,
            passes=20,
            alpha="auto"
        )

        # gensimì€ log perplexity ë°˜í™˜
        log_perplexity = lda.log_perplexity(corpus)
        perplexity = np.exp(-log_perplexity)

        topic_nums.append(k)
        perplexity_scores.append(perplexity)

        print(f"k={k:2d} | Perplexity={perplexity:.2f}")

    return topic_nums, perplexity_scores


# %%
topic_nums, perplexity_scores = compute_perplexity_only(
    dictionary=dictionary,
    corpus=corpus,
    start=2,
    limit=15,
    step=1
)


# %%
optimal_k = topic_nums[np.argmin(perplexity_scores)]
min_perplexity = min(perplexity_scores)

print(f"ìµœì†Œ Perplexity: {min_perplexity:.2f} (í† í”½ ìˆ˜ = {optimal_k})")


# %%
import matplotlib.pyplot as plt

plt.figure(figsize=(8, 5))
plt.plot(topic_nums, perplexity_scores, marker="o")
plt.xlabel("Number of Topics")
plt.ylabel("Perplexity")
plt.title("Perplexity by Number of Topics")
plt.xticks(topic_nums)
plt.grid(True)
plt.show()


# %%

perplexity_df = pd.DataFrame({
    "num_topics": topic_nums,
    "perplexity": perplexity_scores
})

perplexity_df.to_csv(
    "ë°ì´í„°/lda_perplexity.csv",
    index=False,
    encoding="utf-8-sig"
)


# %% [markdown]
# í† í”½ ë‚´ ë‹¨ì–´ ìˆ˜ ê²°ì •

# %%


topn = 15

rows = []
for topic_id in range(lda_model.num_topics):
    topic_terms = lda_model.show_topic(topic_id, topn=topn)
    for rank, (word, prob) in enumerate(topic_terms, start=1):
        rows.append({
            "topic": f"Topic {topic_id+1}",
            "rank": rank,
            "term": word,
            "probability": prob
        })

topic_terms_df_round = pd.DataFrame(rows).sort_values(["topic","rank"]).reset_index(drop=True)
topic_terms_df_round["cum_prob"] = topic_terms_df_round.groupby("topic")["probability"].cumsum()



# %%
topic_terms_df_round.groupby("topic")["cum_prob"].max().sort_values()


# %%
topic_terms_df_round["cum_prob"] = (
    topic_terms_df_round
    .groupby("topic")["probability"]
    .cumsum()
)

topic_terms_df_round


# %% [markdown]
# ì‹œê°í™”

# %%

# num_topics = 4   # coherenceë¡œ ê²°ì •í•œ ê°’

# lda_model = models.LdaModel(
#     corpus=corpus,
#     id2word=dictionary,
#     num_topics=num_topics,
#     random_state=42,
#     passes=20,
#     alpha="auto"
# )


# %%

print(lda_model)


# %%
pip install "pandas==1.5.3"


# %%
import pandas as pd, pyLDAvis
print("pandas:", pd.__version__)
print("pyLDAvis:", pyLDAvis.__version__)


# %%
import pyLDAvis
import pyLDAvis.gensim_models as gensimvis
from IPython.display import display

vis_data = gensimvis.prepare(lda_model, corpus, dictionary, sort_topics=False)

pyLDAvis.enable_notebook()
display(vis_data)



# %% [markdown]
# ğŸ”¹ (A) í† í”½ë³„ ìƒìœ„ ë‹¨ì–´ Bar Chart
# 
# â€œê° í† í”½ì„ êµ¬ì„±í•˜ëŠ” í•µì‹¬ ë‹¨ì–´ëŠ” ë¬´ì—‡ì¸ê°€â€

# %%
import matplotlib.pyplot as plt

def plot_top_words(lda_model, dictionary, topic_id, topn=10):
    words_probs = lda_model.show_topic(topic_id, topn=topn)
    words, probs = zip(*words_probs)

    plt.figure(figsize=(6,4))
    plt.barh(words, probs)
    plt.gca().invert_yaxis()
    plt.title(f"Topic {topic_id+1}")
    plt.xlabel("Probability")
    plt.show()

# ì˜ˆì‹œ
plot_top_words(lda_model, dictionary, topic_id=0)


# %% [markdown]
# ğŸ”¹ (B) ëª¨ë“  í† í”½ í•œ ë²ˆì— (Subplot)

# %%
fig, axes = plt.subplots(lda_model.num_topics, 1, figsize=(6, 2*lda_model.num_topics))

for i, ax in enumerate(axes):
    words_probs = lda_model.show_topic(i, topn=8)
    words, probs = zip(*words_probs)
    ax.barh(words, probs)
    ax.set_title(f"Topic {i+1}")
    ax.invert_yaxis()

plt.tight_layout()
plt.show()


# %%
doc_topics = [
    [prob for _, prob in lda_model.get_document_topics(bow, minimum_probability=0)]
    for bow in corpus
]

doc_topic_df = pd.DataFrame(doc_topics)

doc_topic_df.head()


# %% [markdown]
# 2ï¸âƒ£ ë¬¸ì„œâ€“í† í”½ ê´€ê³„ ì‹œê°í™” (í•´ì„ìš©ìœ¼ë¡œ ë§¤ìš° ì¤‘ìš”)
# ğŸ”¹ (C) ë¬¸ì„œë³„ ì£¼ìš” í† í”½ ë¶„í¬ (Stacked Bar)
# 
# â€œë¬¸ì„œë“¤ì€ ì–´ë–¤ í† í”½ ì¡°í•©ìœ¼ë¡œ êµ¬ì„±ë˜ëŠ”ê°€â€

# %%
doc_topic_df.iloc[:20].plot(
    kind="bar",
    stacked=True,
    figsize=(10,4),
    legend=False
)
plt.ylabel("Topic proportion")
plt.title("Documentâ€“Topic Distribution (Sample)")
plt.show()


# %% [markdown]
# ğŸ”¹ (D) ê° í† í”½ì´ ì°¨ì§€í•˜ëŠ” í‰ê·  ë¹„ì¤‘

# %%
topic_prevalence = doc_topic_df.mean()

topic_prevalence.plot(
    kind="bar",
    figsize=(7,4)
)
plt.ylabel("Average topic proportion")
plt.title("Topic Prevalence Across Documents")
plt.show()


# %% [markdown]
# 3ï¸âƒ£ í† í”½ ê°„ ìœ ì‚¬ë„ / êµ¬ì¡° ì‹œê°í™” (ê³ ê¸‰)
# ğŸ”¹ (E) í† í”½ ê°„ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ Heatmap
# 
# â€œí† í”½ë“¤ì´ ì„œë¡œ ì–¼ë§ˆë‚˜ ê²¹ì¹˜ëŠ”ê°€â€

# %%
from sklearn.metrics.pairwise import cosine_similarity

topic_word_matrix = lda_model.get_topics()
similarity = cosine_similarity(topic_word_matrix)

plt.figure(figsize=(6,5))
plt.imshow(similarity, cmap="viridis")
plt.colorbar()
plt.title("Topic Similarity (Cosine)")
plt.xlabel("Topic")
plt.ylabel("Topic")
plt.show()


# %% [markdown]
# ğŸ”¹ (F) PCA / t-SNEë¡œ í† í”½ ìœ„ì¹˜ ì‹œê°í™”

# %%
from sklearn.decomposition import PCA

pca = PCA(n_components=2)
coords = pca.fit_transform(topic_word_matrix)

plt.figure(figsize=(6,5))
plt.scatter(coords[:,0], coords[:,1])

for i in range(len(coords)):
    plt.text(coords[i,0], coords[i,1], f"T{i+1}")

plt.title("Topic Map (PCA)")
plt.xlabel("PC1")
plt.ylabel("PC2")
plt.show()


# %%

# import matplotlib.pyplot as plt

# =========================================================
# Figure 1(a) Average Topic Prevalence
#  + Figure 1(b) Dominant Topic Distribution
# =========================================================

K = lda_model.num_topics

# 1) ë¬¸ì„œ-í† í”½ ë¶„í¬ í–‰ë ¬ ë§Œë“¤ê¸° (N_docs x K)
doc_topic = np.zeros((len(corpus), K), dtype=float)
for i, bow in enumerate(corpus):
    for tid, prob in lda_model.get_document_topics(bow, minimum_probability=0):
        doc_topic[i, tid] = prob

doc_topic_df = pd.DataFrame(
    doc_topic,
    columns=[f"Topic {i+1}" for i in range(K)]
)

# 2) (a) í‰ê·  í† í”½ ë¹„ì¤‘
avg_prevalence = doc_topic_df.mean(axis=0).values  # ê¸¸ì´ K

# 3) (b) ì§€ë°°ì  í† í”½(ê° ë¬¸ì„œì—ì„œ ê°€ì¥ í° í† í”½) ë¹ˆë„/ë¹„ìœ¨
dominant_topic_idx = doc_topic_df.values.argmax(axis=1)  # 0..K-1
dominant_counts = pd.Series(dominant_topic_idx).value_counts().sort_index()
dominant_counts = dominant_counts.reindex(range(K), fill_value=0)

dominant_ratio = dominant_counts / len(corpus)

# 4) Figure 1(a)+(b) ê·¸ë¦¬ê¸°
fig, axes = plt.subplots(1, 2, figsize=(12, 4))

# ---- (a) Average Topic Prevalence ----
axes[0].bar(range(1, K+1), avg_prevalence)
axes[0].set_title("Figure 1(a). Average Topic Prevalence")
axes[0].set_xlabel("Topic")
axes[0].set_ylabel("Average topic proportion")
axes[0].set_xticks(range(1, K+1))
axes[0].grid(True, axis="y", alpha=0.3)

# ---- (b) Dominant Topic Distribution ----
axes[1].bar(range(1, K+1), dominant_ratio.values)
axes[1].set_title("Figure 1(b). Dominant Topic Distribution")
axes[1].set_xlabel("Topic")
axes[1].set_ylabel("Proportion of documents (dominant topic)")
axes[1].set_xticks(range(1, K+1))
axes[1].grid(True, axis="y", alpha=0.3)

plt.tight_layout()
plt.show()

# 5) (ì„ íƒ) í‘œë¡œ ì €ì¥ (ë…¼ë¬¸ í‘œ/ë¶€ë¡/ê²€ì¦ìš©)
out_df = pd.DataFrame({
    "topic": [f"Topic {i+1}" for i in range(K)],
    "avg_prevalence": avg_prevalence,
    "dominant_doc_count": dominant_counts.values,
    "dominant_doc_ratio": dominant_ratio.values
})

out_df


# %% [markdown]
# A) í† í”½ë³„ ë¹„ì¤‘ ë³€í™”: ì—°ë„ë³„ Topic Prevalence Over Time
# 1) ì—°ë„ ì»¬ëŸ¼ ìë™ íƒìƒ‰ + ì •ë¦¬
# 
# Scopus ì—‘ì…€ì€ ë³´í†µ Year, Publication Year, Year Published ê°™ì€ ì´ë¦„ì´ ë§ì•„ì„œ ìë™ìœ¼ë¡œ ì°¾ì•„ ì”ë‹ˆë‹¤.

# %%

# 1) ì—°ë„ ì»¬ëŸ¼ ì°¾ê¸° (ê°€ëŠ¥í•œ í›„ë³´ë“¤)
year_candidates = [
    "Year", "Publication Year", "Year Published", "PubYear", "PY", "ì¶œíŒì—°ë„", "ì—°ë„"
]

year_col = None
for c in year_candidates:
    if c in df.columns:
        year_col = c
        break

if year_col is None:
    raise ValueError(f"ì—°ë„ ì»¬ëŸ¼ì„ ëª» ì°¾ì•˜ì–´ìš”. df.columnsì—ì„œ ì—°ë„ ì»¬ëŸ¼ëª…ì„ í™•ì¸í•´ ì£¼ì„¸ìš”.\n{list(df.columns)}")

print("ì—°ë„ ì»¬ëŸ¼:", year_col)

# 2) ì—°ë„ ì •ìˆ˜í™”
years = pd.to_numeric(df[year_col], errors="coerce").astype("Int64")


# %%
[c for c in df.columns if "year" in c.lower() or "date" in c.lower()]


# %% [markdown]
# 

# %%

# âœ… ë¬¸ì„œ(í–‰) ìˆ˜ì™€ ë™ì¼í•œ ê¸¸ì´ë¡œ years ìƒì„±
years = pd.to_numeric(df[year_col], errors="coerce")

print("ë¬¸ì„œ ìˆ˜:", len(df), " / years ê¸¸ì´:", len(years))
years.head()


# %%
years.value_counts().sort_index()


# %% [markdown]
# 2) ì—°ë„ë³„ í‰ê·  í† í”½ ë¹„ì¤‘ ê³„ì‚° + ë¼ì¸ í”Œë¡¯

# %%
# =========================================================
# ì—°ë„ë³„ í† í”½ ë¹„ì¤‘ (Topic Prevalence Over Time)
# - ì»¬ëŸ¬â€“í† í”½ ëŒ€ì‘ ëª…í™•
# - ì •í™•í•œ ìˆ˜ì¹˜(yearly) í™•ì¸ ê°€ëŠ¥
# =========================================================

# 1) doc_topic_dfì™€ ì—°ë„ ê²°í•©
dt = doc_topic_df.copy()
dt["year"] = years

# 2) ê²°ì¸¡ ì—°ë„ ì œê±° + ì •ìˆ˜í˜• ë³€í™˜
dt = dt.dropna(subset=["year"])
dt["year"] = dt["year"].astype(int)

# 3) ì—°ë„ë³„ í‰ê·  í† í”½ ë¹„ì¤‘ ê³„ì‚°
yearly = (
    dt.groupby("year")[doc_topic_df.columns]
    .mean()
    .sort_index()
)

# 4) ì‹œê°í™”
plt.figure(figsize=(11, 6))

for col in yearly.columns:
    plt.plot(
        yearly.index,
        yearly[col],
        marker="o",
        linewidth=1,
        label=col     # âœ… ì»¬ëŸ¬â€“í† í”½ ëŒ€ì‘ í•µì‹¬
    )

plt.title("Topic Prevalence Over Time (Yearly Mean Topic Proportion)")
plt.xlabel("Year")
plt.ylabel("Mean topic proportion")
plt.grid(True, alpha=0.3)

# âœ… ë²”ë¡€ ì¶”ê°€ (ë…¼ë¬¸ìš© ì •ì„ ìœ„ì¹˜)
plt.legend(
    title="Topics",
    bbox_to_anchor=(1.02, 1),
    loc="upper left",
    frameon=False
)

plt.tight_layout()
plt.show()

# 5) ì •í™•í•œ ë°ì´í„°ê°’ í™•ì¸
yearly.head()

# %%
# 5) ì •í™•í•œ ë°ì´í„°ê°’ í™•ì¸
yearly.head()

yearly.to_excel(
    "yearly_topic_prevalence.xlsx",
    sheet_name="yearly_topic_prevalence"
)


# %% [markdown]
# ğŸ“Œ ë…¼ë¬¸ì—ì„  ë³´í†µ ìƒìœ„ Nê°œ í† í”½ë§Œ ë³´ì—¬ì£¼ëŠ” ê²Œ ë” ê¹”ë”í•©ë‹ˆë‹¤. (ì˜ˆ: í‰ê·  ë¹„ì¤‘ ìƒìœ„ 4ê°œ)

# %%

topN = 4
top_topics = doc_topic_df.mean().sort_values(ascending=False).head(topN).index

plt.figure(figsize=(11, 6))
for col in top_topics:
    plt.plot(yearly.index, yearly[col], marker="o", linewidth=2)

plt.title(f"Top {topN} Topics: Prevalence Over Time")
plt.xlabel("Year")
plt.ylabel("Mean topic proportion")
plt.grid(True, alpha=0.3)
plt.show()

top_topics.tolist()


# %% [markdown]
# B) â€œì£¼ê¸°ì  ìƒìŠ¹/í•˜ë½ íŒ¨í„´â€ ì •ëŸ‰í™”
# 
# ì—°ë„ ë°ì´í„°ëŠ” ë³´í†µ ê³„ì ˆì„±(12ê°œì›”)ì€ ì—†ì§€ë§Œ, â€œì£¼ê¸°ì„±(ëª‡ ë…„ ë‹¨ìœ„ ë“±)â€ ì£¼ì¥ì€ ê°€ëŠ¥í•´ìš”. ë…¼ë¬¸/ë¦¬ë·° ëŒ€ì‘ì— ê°€ì¥ ë¬´ë‚œí•œ ì¡°í•©ì€:
# 
# ì´ë™í‰ê· (ìŠ¤ë¬´ë”©)ìœ¼ë¡œ ì¶”ì„¸ vs ë³€ë™ ë¶„ë¦¬
# 
# ACF(ìê¸°ìƒê´€)ìœ¼ë¡œ ì£¼ê¸° í›„ë³´(ëª‡ ë…„ ë‹¨ìœ„) ì œì‹œ
# 
# 1) ì´ë™í‰ê· (rolling mean)

# %%

topic_to_check = top_topics[0]  # ê°€ì¥ í° í† í”½ í•˜ë‚˜ë¶€í„° ì²´í¬ (ì›í•˜ë©´ ë°”ê¿”ë„ ë¨)

series = yearly[topic_to_check].copy()
roll = series.rolling(window=3, center=True, min_periods=1).mean()  # 3ë…„ ì´ë™í‰ê· 

plt.figure(figsize=(11, 5))
plt.plot(series.index, series.values, marker="o", linewidth=1, label="Yearly mean")
plt.plot(roll.index, roll.values, linewidth=3, label="3-year moving average")
plt.title(f"Trend vs Fluctuation: {topic_to_check}")
plt.xlabel("Year")
plt.ylabel("Mean topic proportion")
plt.grid(True, alpha=0.3)
plt.legend()
plt.show()


# %% [markdown]
# 2) ACF(ìê¸°ìƒê´€) â€” â€œëª‡ ë…„ ì£¼ê¸°â€ íŒíŠ¸
# ğŸ“Œ í•´ì„ í¬ì¸íŠ¸(ë…¼ë¬¸ìš©):
# 
# Lag 2~5ì—ì„œ ì–‘ì˜ ìƒê´€ì´ ë°˜ë³µë˜ë©´ â€œëª‡ ë…„ ë‹¨ìœ„ì˜ ë°˜ë³µì  ë³€ë™ ê°€ëŠ¥ì„±â€ì„ íƒìƒ‰ì ìœ¼ë¡œ ì œì‹œí•  ìˆ˜ ìˆì–´ìš”.
# 
# ë‹¨, í‘œë³¸ì—°ë„ ìˆ˜ê°€ ì ìœ¼ë©´(ì˜ˆ: 10ë…„ ì´í•˜) ê°•í•œ ì£¼ì¥ ê¸ˆì§€í•˜ê³  â€œexploratoryâ€ë¡œ ì“°ëŠ” ê²Œ ì•ˆì „í•©ë‹ˆë‹¤.

# %%

def plot_acf_manual(x, max_lag=10, title="ACF"):
    x = np.asarray(x, dtype=float)
    x = x - np.nanmean(x)
    x = x[~np.isnan(x)]
    n = len(x)
    if n < 3:
        raise ValueError("ACFë¥¼ ê³„ì‚°í•˜ê¸°ì—” ì—°ë„ ë°ì´í„° í¬ì¸íŠ¸ê°€ ë„ˆë¬´ ì ì–´ìš”.")

    acf_vals = []
    for lag in range(max_lag + 1):
        if lag == 0:
            acf_vals.append(1.0)
        else:
            acf_vals.append(np.corrcoef(x[:-lag], x[lag:])[0, 1])

    plt.figure(figsize=(9, 4))
    plt.stem(range(max_lag + 1), acf_vals, use_line_collection=True)
    plt.title(title)
    plt.xlabel("Lag (years)")
    plt.ylabel("Autocorrelation")
    plt.grid(True, alpha=0.3)
    plt.show()

plot_acf_manual(series.values, max_lag=10, title=f"ACF: {topic_to_check}")


# %% [markdown]
# C) í† í”½ ê°„ ìœ ì‚¬ë„ ë§¤íŠ¸ë¦­ìŠ¤ (Topic Similarity Matrix)
# 
# ê°€ì¥ ë³´í¸ì ì´ê³  ë¦¬ë·°ì–´ ì„¤ë“ì— ë°”ë¡œ ë¨¹í™ë‹ˆë‹¤.
# 
# 1) í† í”½-ë‹¨ì–´ ë¶„í¬ í–‰ë ¬ ê¸°ë°˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„

# %%


topic_word = lda_model.get_topics()   # (K x V) í† í”½-ë‹¨ì–´ í™•ë¥ í–‰ë ¬
sim = cosine_similarity(topic_word)  # (K x K)

plt.figure(figsize=(6.5, 5.5))
plt.imshow(sim)
plt.colorbar()
plt.title("Topic Similarity Matrix (Cosine similarity)")
plt.xticks(range(K), [f"T{i+1}" for i in range(K)], rotation=45, ha="right")
plt.yticks(range(K), [f"T{i+1}" for i in range(K)])
plt.grid(False)
plt.show()

sim_df = pd.DataFrame(sim, index=[f"T{i+1}" for i in range(K)], columns=[f"T{i+1}" for i in range(K)])
sim_df.round(3).head()


# %% [markdown]
# 1ï¸âƒ£ í† í”½ë³„ ì—°ë„ ë³€í™” ê¸°ìš¸ê¸°(slope) ê³„ì‚°
# 
# ë‹¨ìˆœì„ í˜•íšŒê·€: topic_prevalence=Î²0â€‹+Î²1â€‹â‹…year

# %%

years = yearly.index.values.astype(float)

slope_rows = []

for topic in yearly.columns:
    y = yearly[topic].values.astype(float)

    # ê²°ì¸¡ ì œê±°
    mask = ~np.isnan(y)
    x = years[mask]
    y = y[mask]

    if len(x) < 3:
        slope = np.nan
        r2 = np.nan
    else:
        # 1ì°¨ íšŒê·€
        coef = np.polyfit(x, y, 1)
        slope = coef[0]

        # R^2 ê³„ì‚° (ì„¤ëª…ë ¥ ë³´ì¡° ì§€í‘œ)
        y_hat = np.polyval(coef, x)
        ss_res = np.sum((y - y_hat) ** 2)
        ss_tot = np.sum((y - np.mean(y)) ** 2)
        r2 = 1 - ss_res / ss_tot if ss_tot > 0 else np.nan

    slope_rows.append({
        "topic": topic,
        "slope": slope,
        "r2": r2
    })

slope_df = pd.DataFrame(slope_rows)
slope_df


# %%

slope_df_sorted = slope_df.sort_values("slope", ascending=False)
slope_df_sorted


# %%

plt.figure(figsize=(8, 4.5))

colors = ["#1f77b4" if s > 0 else "#d62728" for s in slope_df_sorted["slope"]]

plt.bar(
    slope_df_sorted["topic"],
    slope_df_sorted["slope"],
    color=colors
)

plt.axhline(0, color="black", linewidth=0.8)
plt.ylabel("Slope (change in topic prevalence per year)")
plt.xlabel("Topic")
plt.title("Yearly Trend of Topic Prevalence (Linear Slope)")
plt.xticks(rotation=45, ha="right")
plt.grid(True, axis="y", alpha=0.3)
plt.tight_layout()
plt.show()


# %%

slope_df_sorted["slope_z"] = (
    (slope_df_sorted["slope"] - slope_df_sorted["slope"].mean())
    / slope_df_sorted["slope"].std()
)

slope_df_sorted


# %%
!pip install statsmodels

# %%

import statsmodels.api as sm

years = yearly.index.values.astype(float)

rows = []

for topic in yearly.columns:
    y = yearly[topic].values.astype(float)

    # ê²°ì¸¡ ì œê±°
    mask = ~np.isnan(y)
    x = years[mask]
    y = y[mask]

    if len(x) < 3:
        rows.append({
            "topic": topic,
            "slope": np.nan,
            "p_value": np.nan,
            "r2": np.nan,
            "n_years": len(x)
        })
        continue

    X = sm.add_constant(x)  # [1, year]
    model = sm.OLS(y, X).fit()

    rows.append({
        "topic": topic,
        "slope": model.params[1],          # Î²1
        "p_value": model.pvalues[1],       # p-value for slope
        "r2": model.rsquared,              # RÂ²
        "n_years": int(model.nobs)
    })

reg_df = pd.DataFrame(rows)
reg_df


# %%

import matplotlib.pyplot as plt
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.manifold import MDS

# 1) í† í”½-ë‹¨ì–´ ë¶„í¬
topic_word = lda_model.get_topics()   # (K x V)

# 2) ì½”ì‚¬ì¸ ìœ ì‚¬ë„ â†’ ê±°ë¦¬
sim = cosine_similarity(topic_word)
dist = 1 - sim

# 3) MDSë¡œ 2ì°¨ì› ë°°ì¹˜
mds = MDS(
    n_components=2,
    dissimilarity="precomputed",
    random_state=42
)
coords = mds.fit_transform(dist)

coords_df = pd.DataFrame(
    coords,
    columns=["x_struct", "y_struct"]
)
coords_df["topic"] = [f"Topic {i+1}" for i in range(K)]

coords_df


# %%

plot_df = coords_df.merge(reg_df, on="topic", how="left")
plot_df


# %% [markdown]
# Slope vs Similarity 2D ë§µ ê·¸ë¦¬ê¸° (ë…¼ë¬¸ìš© Figure)
# 
# â‘  ì¦ê°€/ê°ì†Œì˜ ë°©í–¥ì„±
# 
# Xì¶• ê¸°ì¤€ìœ¼ë¡œ í† í”½ì´ ì¦ê°€êµ° / ê°ì†Œêµ°ìœ¼ë¡œ ëª…í™•íˆ ë¶„ë¦¬ë¨
# 
# â‘¡ êµ¬ì¡°ì  êµ°ì§‘
# 
# Yì¶•ì—ì„œ ê°€ê¹Œìš´ í† í”½ë“¤ì€ ìœ ì‚¬í•œ ë‹¨ì–´ ë¶„í¬
# 
# â€œì¦ê°€í•˜ëŠ” í† í”½ì´ íŠ¹ì • êµ¬ì¡° ì˜ì—­ì— ì§‘ì¤‘ë˜ëŠ”ì§€â€ ë…¼ì˜ ê°€ëŠ¥
# 
# â‘¢ ì˜ë¯¸ ìˆëŠ” ë³€í™”ë§Œ ê°•ì¡°
# 
# p < .05 í† í”½ â†’ ì§„í•˜ê²Œ í‘œì‹œ
# 
# ë¹„ìœ ì˜ í† í”½ â†’ ë°˜íˆ¬ëª… ì²˜ë¦¬

# %%

plt.figure(figsize=(8, 6))

for _, row in plot_df.iterrows():
    color = "#1f77b4" if row["slope"] > 0 else "#d62728"
    alpha = 0.9 if row["p_value"] < 0.05 else 0.5  # ìœ ì˜í•œ í† í”½ ê°•ì¡°

    plt.scatter(
        row["slope"],
        row["y_struct"],
        color=color,
        s=120,
        alpha=alpha
    )

    plt.text(
        row["slope"],
        row["y_struct"],
        row["topic"].replace("Topic ", "T"),
        fontsize=10,
        ha="center",
        va="center",
        color="white" if alpha > 0.7 else "black",
        bbox=dict(boxstyle="round,pad=0.25", fc=color, alpha=alpha)
    )

plt.axvline(0, color="black", linestyle="--", linewidth=0.8)

plt.xlabel("Temporal Trend (Slope)")
plt.ylabel("Structural Position (Topic Similarity)")
plt.title("Topic Dynamics Map: Temporal Trend vs Structural Similarity")

plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()


# %%

import numpy as np

# =========================================================
# 1ï¸âƒ£ ë°ì´í„° ë³µì‚¬
# =========================================================
df_year = df.copy()

# =========================================================
# 2ï¸âƒ£ ì—°ë„ ì»¬ëŸ¼ ì •ë¦¬
# =========================================================
df_year["year"] = pd.to_numeric(df_year["Year"], errors="coerce")
df_year = df_year.dropna(subset=["year"])
df_year["year"] = df_year["year"].astype(int)

# =========================================================
# 3ï¸âƒ£ 1974ë…„ ê¸°ì¤€ 5ë…„ ë‹¨ìœ„ êµ¬ê°„ ìƒì„±
# =========================================================
START_YEAR = 1971
BIN_SIZE = 5
END_YEAR = 2025

# âœ… 2025ë…„ê¹Œì§€ë§Œ í•„í„°ë§
df_year = df_year[df_year["year"] <= END_YEAR]

df_year["period_start"] = (
    START_YEAR + ((df_year["year"] - START_YEAR) // BIN_SIZE) * BIN_SIZE

)

df_year["period_label"] = (
    df_year["period_start"].astype(str)
    + "-"
    + (df_year["period_start"] + BIN_SIZE - 1).astype(str)
)

# =========================================================
# 4ï¸âƒ£ 5ë…„ ë‹¨ìœ„ ë…¼ë¬¸ ìˆ˜
# =========================================================
paper_counts_5y = (
    df_year
    .groupby("period_label")
    .size()
    .sort_index()
)

# =========================================================
# 5ï¸âƒ£ 5ë…„ ë‹¨ìœ„ ì¦ê°(Î”)
# =========================================================
paper_change_5y = paper_counts_5y.diff()

# =========================================================
# 6ï¸âƒ£ ê²°ê³¼ ì¶œë ¥
# =========================================================
paper_counts_5y, paper_change_5y


# %%
# =========================================================
# 1ï¸âƒ£ 5ë…„ ë‹¨ìœ„ ì§‘ê³„ í…Œì´ë¸” ìƒì„±
# =========================================================
table_5y = paper_counts_5y.reset_index()
table_5y.columns = ["Period", "Paper_Count"]

# =========================================================
# 2ï¸âƒ£ ì „ë…„ë¹„(ì´ì „ êµ¬ê°„ ëŒ€ë¹„ ì¦ê°ë¥ , %) ê³„ì‚°
# =========================================================
table_5y["YoY_%"] = table_5y["Paper_Count"].pct_change() * 100

# =========================================================
# 3ï¸âƒ£ ë³´ê¸° ì¢‹ê²Œ ë°˜ì˜¬ë¦¼
# =========================================================
table_5y["YoY_%"] = table_5y["YoY_%"].round(2)

table_5y


# %%
table_5y.to_csv("paper_count_5year_yoy.csv", index=False, encoding="utf-8-sig")


# %% [markdown]
# 

# %%
plt.figure(figsize=(9, 5))
plt.plot(paper_counts_5y.index, paper_counts_5y.values, marker="o", linewidth=2)
plt.title("Number of Publications by 5-Year Period")
plt.xlabel("5-Year Period")
plt.ylabel("Number of Publications")
plt.xticks(rotation=45)
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()


# %%

# ë¬¸ì„œë³„ í† í”½ ë¶„í¬ í–‰ë ¬ ìƒì„± (ë¬¸ì„œ ìˆ˜ Ã— í† í”½ ìˆ˜)
doc_topic_dist = np.array([
    [prob for _, prob in lda_model.get_document_topics(doc, minimum_probability=0)]
    for doc in corpus
])



# %%
# í† í”½ë³„ í‰ê·  ë¹„ì¤‘ ê³„ì‚°
topic_share = doc_topic_dist.mean(axis=0)

# ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ì •ë¦¬
df_topic_share = pd.DataFrame({
    "Topic": [f"Topic {i+1}" for i in range(len(topic_share))],
    "Proportion": topic_share
})

# ì •ë ¬ (ì„ íƒ)
df_topic_share = df_topic_share.sort_values("Proportion", ascending=False)

df_topic_share


# %%
plt.figure(figsize=(8, 5))
plt.bar(df_topic_share["Topic"], df_topic_share["Proportion"])
plt.ylabel("Average Topic Proportion")
plt.xlabel("Topic")
plt.title("Overall Topic Proportions Across All Documents")
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()


# %% [markdown]
# ê·¸ë¦¼ X. ì „ì²´ ë¬¸ì„œì—ì„œ í† í”½ë³„ í‰ê·  ë¹„ì¤‘ì˜ ëˆ„ì  ë¶„í¬
# (ê° ë¬¸ì„œì˜ í† í”½ í™•ë¥ ì„ í‰ê· í•˜ì—¬ ì‚°ì¶œ)

# %%


# =========================================================
# 1ï¸âƒ£ í† í”½ ë¹„ì¤‘ ê³„ì‚°
# =========================================================
topic_share = doc_topic_dist.mean(axis=0)

df_topic_share = pd.DataFrame({
    "Topic": [f"Topic {i+1}" for i in range(len(topic_share))],
    "Proportion": topic_share
})

# =========================================================
# 2ï¸âƒ£ ëˆ„ì  ë§‰ëŒ€ + í¼ì„¼í‹°ì§€ í‘œì‹œ
# =========================================================
plt.figure(figsize=(10, 2))

left = 0
colors = plt.cm.tab10.colors

for i, row in df_topic_share.iterrows():
    width = row["Proportion"]
    percent = width * 100

    plt.barh(
        ["All Documents"],
        width,
        left=left,
        color=colors[i % len(colors)],
        label=row["Topic"]
    )

    # í¼ì„¼í‹°ì§€ í…ìŠ¤íŠ¸ (ê°€ìš´ë° ë°°ì¹˜)
    plt.text(
        left + width / 2,
        0,
        f"{percent:.1f}%",
        va="center",
        ha="center",
        fontsize=10,
        color="white" if percent > 7 else "black"
    )

    left += width

plt.xlabel("Proportion (100%)")
plt.title("Cumulative Topic Proportions Across All Documents")

plt.legend(
    loc="upper center",
    bbox_to_anchor=(0.5, -0.45),
    ncol=len(df_topic_share)
)

plt.tight_layout()
plt.show()



